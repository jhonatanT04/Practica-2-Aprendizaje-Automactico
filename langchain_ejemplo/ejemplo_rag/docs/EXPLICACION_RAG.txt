================================================================================
                    EXPLICACIÓN DEL EJEMPLO RAG (Retrieval-Augmented Generation)
================================================================================

¿QUÉ ES RAG?
------------
RAG combina la recuperación de información (Retrieval) con la generación de texto (Generation).
En lugar de que el LLM responda solo con su conocimiento entrenado, primero busca información
relevante en tus documentos y luego genera una respuesta basada en ese contexto específico.

FLUJO DEL SISTEMA:
------------------
1. Usuario hace una pregunta
2. El sistema busca en la base de datos vectorial los fragmentos más relevantes
3. Esos fragmentos se insertan como "contexto" en el prompt
4. El LLM genera una respuesta basada ÚNICAMENTE en ese contexto
5. Se devuelve la respuesta al usuario

COMPONENTES CLAVE:
------------------

1. DOCUMENT LOADER (TextLoader)
   - Carga archivos de texto desde el disco
   - Soporta diferentes codificaciones (utf-8, etc.)

2. TEXT SPLITTER (RecursiveCharacterTextSplitter)
   - Divide documentos grandes en fragmentos pequeños (chunks)
   - chunk_size: tamaño máximo de cada fragmento
   - chunk_overlap: superposición entre fragmentos para mantener contexto

3. EMBEDDINGS (VertexAIEmbeddings)
   - Convierte texto en vectores numéricos
   - Textos similares tienen vectores similares
   - Permite búsquedas semánticas (por significado, no solo palabras exactas)

4. VECTOR STORE (FAISS)
   - Base de datos que almacena los vectores
   - Permite búsquedas rápidas de similitud
   - FAISS es en memoria (otros como Pinecone son persistentes)

5. RETRIEVER
   - Interfaz para buscar documentos relevantes
   - Recibe una pregunta, devuelve los fragmentos más similares

6. LLM (ChatVertexAI)
   - El modelo de lenguaje que genera las respuestas
   - En este caso: Gemini 2.5 Pro

7. PROMPT TEMPLATE
   - Define CÓMO el LLM debe usar el contexto
   - Incluye instrucciones claras sobre qué hacer si no sabe la respuesta

8. CHAIN (LCEL)
   - Encadena todos los componentes usando el operador | (pipe)
   - {"context": retriever, "question": RunnablePassthrough()}
     * retriever busca fragmentos relevantes para la pregunta
     * RunnablePassthrough() pasa la pregunta sin modificar
   - prompt inserta contexto y pregunta en la plantilla
   - llm genera la respuesta
   - StrOutputParser() convierte la respuesta a string limpio

VENTAJAS DE RAG:
----------------
✓ Respuestas basadas en tus documentos específicos
✓ Reduce alucinaciones (inventar información)
✓ No requiere reentrenar el modelo
✓ Actualizable (añade documentos sin cambiar el LLM)
✓ Transparente (sabes de dónde vino la información)

CASOS DE USO:
-------------
- Chatbots de soporte técnico (manuales, documentación)
- Asistentes legales (contratos, leyes)
- Análisis de investigación (papers, artículos)
- Bases de conocimiento empresarial
- Tutores educativos personalizados

LIMITACIONES:
-------------
- Calidad depende de los documentos fuente
- Chunks muy pequeños pierden contexto
- Chunks muy grandes dificultan la búsqueda precisa
- Requiere buenos embeddings para búsquedas efectivas
